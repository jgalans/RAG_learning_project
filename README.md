# Proyecto RAG - Sistema de RecomendaciÃ³n de Productos

Un sistema inteligente de bÃºsqueda y recomendaciÃ³n de productos basado en **Retrieval-Augmented Generation (RAG)** que utiliza embeddings vectoriales y un modelo de lenguaje (LLM) para proporcionar recomendaciones personalizadas.

## ğŸ¯ Â¿QuÃ© es este proyecto?

Este proyecto implementa un asistente de IA para una tienda que:

1. **Entiende preguntas en lenguaje natural** - No necesita palabras clave exactas
2. **Busca productos relevantes** - Utiliza embeddings para encontrar similitudes semÃ¡nticas
3. **Genera respuestas conversacionales** - Un LLM proporciona recomendaciones personalizadas

**Ejemplo:**
```
Usuario: "Busco ropa para hacer deporte cuando hace sol"
Sistema: Encuentra la Gorra Running y la Camiseta Eco
LLM: "Te recomiendo la Gorra Running porque tiene material transpirable 
que mantiene tu cabeza fresca. La Camiseta Eco es perfecta porque..."
```

## ğŸ”§ TecnologÃ­as Utilizadas

- **Embeddings**: `sentence-transformers` (all-MiniLM-L6-v2)
- **Base de Datos Vectorial**: ChromaDB
- **Framework**: LangChain
- **LLM**: Ollama (Llama2)
- **Datos**: Pandas

## ğŸ“‹ Requisitos Previos

- Python 3.9+
- Ollama instalado (para el LLM)
- Homebrew (opcional, pero recomendado en macOS)

## ğŸš€ InstalaciÃ³n

### 1. Clonar el repositorio

```bash
git clone https://github.com/tu-usuario/ProyectoRAG.git
cd ProyectoRAG
```

### 2. Crear un entorno virtual

```bash
python -m venv .venv
source .venv/bin/activate  # En macOS/Linux
# o
.venv\Scripts\activate  # En Windows
```

### 3. Instalar dependencias

```bash
pip install -r requirements.txt
```

### 4. Instalar Ollama

En macOS con Homebrew:
```bash
brew install ollama
```

O descÃ¡rgalo desde: https://ollama.ai

### 5. Descargar el modelo Llama2

Abre una terminal y ejecuta:
```bash
ollama pull llama2
```

Luego inicia el servidor de Ollama (en otra terminal):
```bash
ollama serve
```

## ğŸ’» Uso

Ejecuta el script principal:

```bash
python ia_tienda.py
```

El script harÃ¡ una bÃºsqueda de ejemplo y mostrarÃ¡:
- Los productos encontrados
- Una recomendaciÃ³n generada por el LLM

## ğŸ“ Estructura del Proyecto

```
ProyectoRAG/
â”œâ”€â”€ ia_tienda.py          # Script principal con el sistema RAG
â”œâ”€â”€ setup_db.py           # Script para inicializar la BD (si aplica)
â”œâ”€â”€ requirements.txt      # Dependencias del proyecto
â”œâ”€â”€ ecommerce.db          # Base de datos SQLite (generada)
â”œâ”€â”€ data/                 # Carpeta con datos de productos
â””â”€â”€ README.md             # Este archivo
```

## ğŸ” Â¿CÃ³mo funciona?

### Paso 1: Embeddings
Las descripciones de productos se convierten a vectores numÃ©ricos usando `sentence-transformers`

### Paso 2: BÃºsqueda Vectorial
ChromaDB busca los `k` productos mÃ¡s similares al query del usuario usando similitud coseno

### Paso 3: LLM
Ollama (Llama2) toma los productos encontrados y genera una respuesta natural y personalizada

## ğŸ“ Ejemplo de Uso

```python
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Crear embeddings
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Crear base de datos vectorial
textos = ["Producto 1", "Producto 2", "Producto 3"]
vectorstore = Chroma.from_texts(textos, embeddings)

# Buscar productos similares
pregunta = "Tu pregunta aquÃ­"
resultados = vectorstore.similarity_search(pregunta, k=2)

# Generar respuesta con LLM
llm = Ollama(model="llama2")
# ... crear prompt y chain ...
respuesta = chain.run(...)
```

## ğŸ¤ Contribuciones

Â¡Las contribuciones son bienvenidas! SiÃ©ntete libre de hacer fork del proyecto y enviar pull requests.

---

**Nota**: Este es un proyecto educativo para entender cÃ³mo funcionan los sistemas RAG con LLMs.
EOF
